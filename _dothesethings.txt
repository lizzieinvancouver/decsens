<><><><><><><><><>
Notes to Lizzie 
Dealing with edits
<><><><><><><><><>


<><><><><><><><<><>
Revising for a new journal ...
<><><><><><><><><><>

Regarding pnas reviews:
- Try analyses with Type II regression (reduced major axis) ... as reviewer Keenan et al. 2019 suggest
- Maybe add some note acknowledging we note that BetPen and FagSyl are very different species.... same reviewer was looking for photo versus chill models; perhaps stress that we develop a forcing-only NULL model?
- Say that maybe sensitivity has NOT changed recently... or not much at least?
- Table S1-S2: explain the meaning of the 31-45-60 subheadings in the tables
- From Matt:  Metabolic rates of caterpillars tend to have higher Q10s at cooler temperatures than at warmer temperatures (e.g., Ayres and Scriber from before you were born). I think Arrhenius predicted this attribute for chemical reactions in general.

- Map the Ma data and look at how much data we have across a latitudinal gradient
- Think on Ma data
- Add winegrape data?

See also projects/misc/pep725/pep725check_decsens.R


<><><><><><><><<><>
Work on PLOS Bio revisions 
(Summer 2020, submitted in October 2020 I hope)
<><><><><><><><><><>

Have done ...

-> Address non-identifiability: "Show it's non-identified, assume all coefficients are 0, find two sets of parameters that yield the same data (half the interval lengths and double the coefficients)." [Though this is so obvious I just explained it generally in the supp, but decsensSimsMultCues.R (in decsens repo) does have code for this if needed.]
-> Review chilling -- more or less done, if depressing/confusing (see chillunits.pdf)
-> Changed pre-season length (see 'Pre-season NOTES' below).
-> Add table of species with constant sites across the windows we chose. Then add top two species? 
-> Build up simulations in the supp -- to show differences between estimates of models with true declines and with just statistical artifacts (add photoperiod model in addition to chilling, add greater comparison of estimates ...)
-> Added lots of refs (checked: Ailene wrote, I also checked Hunter and Lechowicz and 1992, which seems good for supporting the constant themal sum thresholds. In my quick look, I didn’t see support from Man and Lu, 2010 for constant thermal sum thresholds but I can look again more carefully. [I took out Man & Lu 2010.]]

Did not do, but okay I think:
-> Think on the slides in Auerbach's defense (some_stat_models.pdf)
-> Add lilacs or some other long-term data or just add more PEP species? 

German phenology data: https://link.springer.com/article/10.1007/s00484-013-0650-2
Russian phenology data: https://www.nature.com/articles/s41597-020-0376-z
Lilac data: https://www.nature.com/articles/sdata201538

+++++++
Pre-season NOTES:

- Currently 1 Mar to 30 Apr:
	- we could shift it back 15 d: 15-ish Feb - 15 Apr 
	- we could make it 15 days shorter 
- BETPEN data range is ~80-140, mean is 110 (~mid to late April)
- FAGSYL data range is ~105-135, mean is 110 (very late April)
- Fu et al. find a 60 day preseason is most common across species (Ext. Data Fig 2) and for BETPEN, though for FAGSYL it looks to be tied for 15 and 30 d. For BETPEN 45 d is runner-up. They seem to actually calculate the temperature window from leaf unfolding based on their preseason length, but I don't think we want to do that. 


Here's what Gusewell did: 
"The optimal preseason for each species, station and time period was determined as the series of 20–120 days (with 10-day steps) preceding the mean date of the phenological event, whose mean temperatures correlate most negatively with phenology. We initially varied both preseason start and preseason end dates to check that the optimal end date was the mean date of the phenological event in most cases, as assumed by our model. We then fixed preseason end at the mean date of the event and selected the optimal start date, which in turn determined preseason length. The optimal preseason start date and the following statistics were determined for the entire period 1970–2012 (“fixed start date”), separately for 1970–1987 and 1995–2012 (“period-specific start dates”), and separately for cold and warm years, provided that data from at least eight years were available in the data subset considered."

"The start of the preseason advanced on average by 7.68  2.14 days between the two periods (z = 3.58, p < .001, Figure 4b). However, preseason start advanced only for nine species (mainly early- and late-spring events), and hardly changed for the others (Figure 4b)."

"Temperature sensitivity did on average not differ significantly between years with warm and cold winters (z = 1.57, p = .12)."


+++++++
Pre-season window notes:

In draft sent in early October, all these refs in the pre-season Supp section were checked:
- Park et al 2019 and Bolmgren et al 2013 -- (Ailene wrote) I think they are good citations to support the 60 day window that we used.
- Kopp 2020 and (Dan wrote) the citation is correct.
- Prevey 2017 (I checked, they wrote "The summer time-window was defined as either average May–June, June, June–July, or June–August mean monthly temperatures per year.")
- Zhang et al. 2015 (Journal of Geophysical Research-Biogeosciences) ... Nacho wrote: they use a 60 day fixed window (they inspect correlations for different length periods, but since those correlate with the 60-day one, they end up choosing a fixed window). So this citation seems correct
- Park et al. 2018 (pre-defined) -- Dan
- Wang et al. 2015 (statistical search) -- Nacho wrote: they use a 2 month fixed window, but change/slide when the window start/ends conditioned on the last month containing the onset of the phenological event. So it should be ok?

Ailene:
I didn't find *the perfect* paper for you to cite supporting the window we chose, but here are three options that are at least well-aligned with ours: and might help:
1) The Park et al 2019 paper "Herbarium specimens reveal substantial and unexpected variation in phenological sensitivity across the eastern United States" in PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES uses Mar-May (almost our window...)
2) Wang et al  2014 "The influence of local spring temperature variance on temperature sensitivity of spring phenology" used 60-day windows that ended on the mean leafout date overtime (so the start date varied by site; they also tested 45- and 90-day windows
3) Bolmgren et al used the 60-day time-period before mean FFD and stated "Using a proxy for 90 days did not improve the fit of the model, while a proxy for 30 days reduced the fit."

Dan:
Wang 2020 (Satellite-observed decrease in the sensitivity of spring phenology to climate change under high nitrogen deposition) used March April and May temps as a window as well.


+++++++
Other NOTES ...

JA wrote about the linear alpha_1 example from PEP

If you want to plot the data, I would consider small multiples (using facet_wrap to give each site its own plot): https://statmodeling.stat.columbia.edu/2014/04/10/small-multiples-lineplots-maps-ok-always-yes-case/

However, I think reporting the R^2 is better—looks like it will be > .95 for all sites. You could compare that to the R^2 of leaf-out on mean temperature since if researchers are willing to assume that that relationship is linear, they would have to agree that ours is as well.

REFS from Dan B on thermal sums ...

His email (1 Sep 2020):

The attached paper indicates that thermal sums have been used to some degree (no pun intended) since the 1700's, however it's a critique of using them so I don't know if it is good to cite that for our purposes. Instead I'm attaching a few other citations from within it that seem like "classics".

Attached paper is Wang  1960: A Critique of the Heat Unit Approach to Plant Response Studies (Ecology)


<><><><><><><><<><>
Notes to myself on friendly review edits:
<><><><><><><><><><>

One point Christy made that I liked and wanted to squeeze in is:

"Another point I’ve had to bring up with reviewers that i think you could hit a bit harder is that there is little biological basis for the default assumption to be linear effects — it’s really just an artifact of our standard statistical toolbox. If we think about the basic biophysical processes that drive our observable states — things like enzyme kinetics or assimilation rates — the basic physics and chemistry of how those things work are inherently non-linear. Is there a good explanation why non-linear fundamental processes should magically become linear in our observed states?"

I decided it is kind of implicit in our argument, and Jonathan also pointed out that assuming linearity for lots of non-linear things can work fine, especially in-sample, so now I feel okay leaving it out. I told Christy she should make it in more papers as it's brilliant. 